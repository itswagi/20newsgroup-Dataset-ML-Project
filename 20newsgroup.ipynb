{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "er such \\n   circumstances, and now the Lebanese government has proven that it is\\n   capable of controlling and disarming all militias as they did\\n   in all other parts of Lebanon.\\n\\nNo, the Syrian gov\\'t is more than happy to have Israel sink into another\\nLebanese morass.  I could elaborate if necessary.\\n\\n   I agree, only in the case of the Isareli soldiers their killing\\n   CANNOT be qualified as murder, no matter what you say.\\n\\nNo, but it is regretable, as is the whole situation.\\n\\n--\\nShai Guday              | Stealth bombers,\\nOS Software Engineer    |\\nThinking Machines Corp. |\\tthe winged ninjas of the skies.\\nCambridge, MA           |\\n', \"From: mrj@cs.su.oz.au (Mark James)\\nSubject: Re: IDE vs SCSI (Why VLB busmastering slows your system)\\nOrganization: Basser Dept of Computer Science, University of Sydney, Australia\\nLines: 12\\n\\nIn article <1993Apr16.205724.26258@mnemosyne.cs.du.edu> smace@nyx.cs.du.edu (Scott Mace) writes:\\n>\\n>Have you ever seen what happens when you hook a busmaster controller to\\n>a vesa local bus.  It actually  slows down your system.....\\n>\\n>If you don't belive what I said about busmastering and vlbus then pick\\n>up a back issue of PC-week in whihc they tested vlbus, eisa and isa\\n>busmastering cards.\\n\\nIs VLB busmastering bad because it stops the processor fetching from\\nexternal cache as well as main memory while the VLB card has the bus?\\nHow significant is the slowing effect?\\n\", \"From: chein@eng.auburn.edu (Tsan Heui)\\nSubject: IN CASE A DEAL IS A LEMON ....\\nNntp-Posting-Host: wilbur.eng.auburn.edu\\nOrganization: Auburn University Engineering\\nDistribution: usa\\nLines: 22\\n\\n\\nHi to all.\\n\\nSince all of you could also be a seller as well as a buyer, I'd like to bring\\nthis issue for discussion - what would be the best solution in case a deal \\nbecame a lemon?\\n\\nAs I understand most people selling things over the net do not grant a warranty,I am in such a situation that the seller did not state whether a warranty would be granted or not and the item I received is out of order. The seller insisted\\nthat it was 'in good condition' when he sent it and so would just return half ofthe amount that I paid if I send the item back to him and after he is sured it\\nis bad. Is this reasonable?\\n\\nBasically I would like to believe the seller tells the truth. Also, I am positively to say that I've not done anything wrong which might cause the failure of \\nthe thing. My assumption here is everyone is honest - so rule out the possibility that either one of the two parties or both are liars.\\n\\nI would like to hear your opinion - either in here or directly respond to my\\ne-mail address.\\n\\nI know there is such a risk that you could lose money. But, how can we make it\\nenjoyable to most people and not wasting the bandwidth?\\n\\nchein\\n\\n\", \"From: adrian@ora.COM (Adrian Nye)\\nSubject: imake book review\\nOrganization: O'Reilly and Associates, Inc.\\nLines: 12\\nReply-To: adrian@ora.com\\nNNTP-Posting-Host: enterpoop.mit.edu\\nTo: xpert@expo.lcs.mit.edu\\n\\n\\nThanks for the many offers to review this book.\\n\\nIf you received a review copy, please return it\\nas soon as possible.  I had a system crash and\\nlost the list of people I sent it to!\\n\\nThanks\\n\\nAdrian Nye\\nO'Reilly and Associates\\nadrian@ora.com\\n\", 'From: jim.zisfein@factory.com (Jim Zisfein) \\nSubject: Re: Migraines and scans\\nDistribution: world\\nOrganization: Invention Factory\\'s BBS - New York City, NY - 212-274-8298v.32bis\\nReply-To: jim.zisfein@factory.com (Jim Zisfein) \\nLines: 37\\n\\nDN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nDN> A neurology\\nDN> consultation is cheaper than a scan.\\n\\nAnd also better, because a neurologist can make a differential\\ndiagnosis between migraine, tension-type headache, cluster, benign\\nintracranial hypertension, chronic paroxysmal hemicrania, and other\\nheadache syndromes that all appear normal on a scan.  A neurologist\\ncan also recommend a course of treatment that is appropriate to the\\ndiagnosis.\\n\\nDN> >>Also, since many people are convinced they have brain tumors or other\\nDN> >>serious pathology, it may be cheaper to just get a CT scan then have\\nDN> >>them come into the ER every few weeks.\\nDN> And easier than taking the time to reassure the patient, right?\\nDN> Personally, I don\\'t think this can ever be justified.\\n\\nSigh.  It may never be justifiable, but I sometimes do it.  Even\\nafter I try to show thoroughness with a detailed history, neurologic\\nexamination, and discussion with the patient about my diagnosis,\\nsalted with lots of reassurance, patients still ask \"why can\\'t you\\norder a scan, so we can be absolutely sure?\"  Aunt Millie often gets\\ninto the conversation, as in \"they ignored Aunt Millie\\'s headaches\\nfor years\", and then she died of a brain tumor, aneurysm, or\\nwhatever.  If you can get away without ever ordering imaging for a\\npatient with an obviously benign headache syndrome, I\\'d like to hear\\nwhat your magic is.\\n\\nEvery once in a while I am able to bypass imaging by getting an EEG.\\nMind you, I don\\'t think EEG is terribly sensitive for brain tumor,\\nbut the patient feels like \"something is being done\" (as if the\\nhours I spent talking with and examining the patient were\\n\"nothing\"), the EEG has no ionizing radiation, it\\'s *much* cheaper\\nthan CT or MRI, and the EEG brings in some money to my department.\\n---\\n . SLMR 2.1 . E-mail: jim.zisfein@factory.com (Jim Zisfein)\\n                                                                         \\n', 'From: ebodin@pearl.tufts.edu\\nSubject: Screen Death: Mac Plus/512\\nLines: 22\\nOrganization: Tufts University - Medford, MA\\n\\nI have a (very old) Mac 512k and a Mac Plus, both of which \\nhave the same problem.\\n\\nTheir screens blank out, sometimes after a minor physical jolt\\n(such as inserting a floppy in the internal drive), sometimes \\nall by themselves (computer left to itself just goes blank).\\n\\nI have replaced the wires connecting the logic boards and the \\nvideo board, because it seemed at first that jiggling the wires\\nmade the screen come back on.  This worked for a while, but the\\nblanking out has returned.\\n\\nCan I do anything?  Do I need a new power supply?  A new CRT?\\nA new computer?\\n\\nThanks for any advice...\\n\\n--------------------------\\nEthan Bodin\\nTufts University\\nebodin@pearl.tufts.edu\\n--------------------------\\n', 'From: westes@netcom.com (Will Estes)\\nSubject: Mounting CPU Cooler in vertical case\\nOrganization: Mail Group\\nX-Newsreader: TIN [version 1.1 PL8]\\nLines: 13\\n\\nI just installed a DX2-66 CPU in a clone motherboard, and tried mounting a CPU \\ncooler on the chip.  After about 1/2 hour, the weight of the cooler was enough \\nto dislodge the CPU from its mount.  It ended up bending a few pins\\non the CPU, but luckily the power was not on yet.  I ended up\\npressing the CPU deeply into its socket and then putting the CPU\\ncooler back on.  So far so good.\\n\\nHave others had this problem?  How do you ensure that the weight of\\nthe CPU fan and heatsink do not eventually work the CPU out of its\\nsocket when mounting the motherboard in a vertical case?\\n\\n-- \\nWill Estes\\t\\tInternet: westes@netcom.com\\n', \"From: steve@hcrlgw (Steven Collins)\\nSubject: Re: Sphere from 4 points?\\nOrganization: Central Research Lab. Hitachi, Ltd.\\nLines: 27\\nNntp-Posting-Host: hcrlgw\\n\\nIn article <1qkgbuINNs9n@shelley.u.washington.edu> bolson@carson.u.washington.edu (Edward Bolson) writes:\\n>Boy, this will be embarassing if it is trivial or an FAQ:\\n>\\n>Given 4 points (non coplanar), how does one find the sphere, that is,\\n>center and radius, exactly fitting those points?  I know how to do it\\n>for a circle (from 3 points), but do not immediately see a \\n>straightforward way to do it in 3-D.  I have checked some\\n>geometry books, Graphics Gems, and Farin, but am still at a loss?\\n>Please have mercy on me and provide the solution?  \\n\\nWouldn't this require a hyper-sphere.  In 3-space, 4 points over specifies\\na sphere as far as I can see.  Unless that is you can prove that a point\\nexists in 3-space that is equi-distant from the 4 points, and this may not\\nnecessarily happen.\\n\\nCorrect me if I'm wrong (which I quite possibly am!)\\n\\nsteve\\n---\\n\\n\\n\\n-- \\n+---------------------------------------+--------------------------------+\\n| Steven Collins\\t\\t\\t| email: steve@crl.hitachi.co.jp |\\n| Visiting Computer Graphics Researcher\\t| phone: (0423)-23-1111 \\t |\\n| Hitachi Central Research Lab. Tokyo.\\t| fax:   (0423)-27-7742\\t\\t |\\n\", \"From: gunning@cco.caltech.edu (Kevin J. Gunning)\\nSubject: stolen CBR900RR\\nOrganization: California Institute of Technology, Pasadena\\nLines: 12\\nDistribution: usa\\nNNTP-Posting-Host: alumni.caltech.edu\\nSummary: see above\\n\\nStolen from Pasadena between 4:30 and 6:30 pm on 4/15.\\n\\nBlue and white Honda CBR900RR california plate KG CBR.   Serial number\\nJH2SC281XPM100187, engine number 2101240.\\n\\nNo turn signals or mirrors, lights taped over for track riders session\\nat Willow Springs tomorrow.  Guess I'll miss it.  :-(((\\n\\nHelp me find my baby!!!\\n\\nkjg\\n\\n\"], 'filenames': array(['C:\\\\Users\\\\waji_\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.autos\\\\102994',\n       'C:\\\\Users\\\\waji_\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\51861',\n       'C:\\\\Users\\\\waji_\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\51879',\n       ...,\n       'C:\\\\Users\\\\waji_\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.ibm.pc.hardware\\\\60695',\n       'C:\\\\Users\\\\waji_\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38319',\n       'C:\\\\Users\\\\waji_\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.motorcycles\\\\104440'],\n      dtype='<U95'), 'target_names': ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'], 'target': array([7, 4, 4, ..., 3, 1, 8]), 'DESCR': '.. _20newsgroups_dataset:\\n\\nThe 20 newsgroups text dataset\\n------------------------------\\n\\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\\n20 topics split in two subsets: one for training (or development)\\nand the other one for testing (or for performance evaluation). The split\\nbetween the train and test set is based upon a messages posted before\\nand after a specific date.\\n\\nThis module contains two loaders. The first one,\\n:func:`sklearn.datasets.fetch_20newsgroups`,\\nreturns a list of the raw texts that can be fed to text feature\\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\\nwith custom parameters so as to extract feature vectors.\\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\\nreturns ready-to-use features, i.e., it is not necessary to use a feature\\nextractor.\\n\\n**Data Set Characteristics:**\\n\\n    =================   ==========\\n    Classes                     20\\n    Samples total            18846\\n    Dimensionality               1\\n    Features                  text\\n    =================   ==========\\n\\nUsage\\n~~~~~\\n\\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\\nfetching / caching functions that downloads the data archive from\\nthe original `20 newsgroups website`_, extracts the archive contents\\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\\n:func:`sklearn.datasets.load_files` on either the training or\\ntesting set folder, or both of them::\\n\\n  >>> from sklearn.datasets import fetch_20newsgroups\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\')\\n\\n  >>> from pprint import pprint\\n  >>> pprint(list(newsgroups_train.target_names))\\n  [\\'alt.atheism\\',\\n   \\'comp.graphics\\',\\n   \\'comp.os.ms-windows.misc\\',\\n   \\'comp.sys.ibm.pc.hardware\\',\\n   \\'comp.sys.mac.hardware\\',\\n   \\'comp.windows.x\\',\\n   \\'misc.forsale\\',\\n   \\'rec.autos\\',\\n   \\'rec.motorcycles\\',\\n   \\'rec.sport.baseball\\',\\n   \\'rec.sport.hockey\\',\\n   \\'sci.crypt\\',\\n   \\'sci.electronics\\',\\n   \\'sci.med\\',\\n   \\'sci.space\\',\\n   \\'soc.religion.christian\\',\\n   \\'talk.politics.guns\\',\\n   \\'talk.politics.mideast\\',\\n   \\'talk.politics.misc\\',\\n   \\'talk.religion.misc\\']\\n\\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\\nattribute is the integer index of the category::\\n\\n  >>> newsgroups_train.filenames.shape\\n  (11314,)\\n  >>> newsgroups_train.target.shape\\n  (11314,)\\n  >>> newsgroups_train.target[:10]\\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\\n\\nIt is possible to load only a sub-selection of the categories by passing the\\nlist of the categories to load to the\\n:func:`sklearn.datasets.fetch_20newsgroups` function::\\n\\n  >>> cats = [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\', categories=cats)\\n\\n  >>> list(newsgroups_train.target_names)\\n  [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train.filenames.shape\\n  (1073,)\\n  >>> newsgroups_train.target.shape\\n  (1073,)\\n  >>> newsgroups_train.target[:10]\\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\\n\\nConverting text to vectors\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn order to feed predictive or clustering models with the text data,\\none first need to turn the text into vectors of numerical values suitable\\nfor statistical analysis. This can be achieved with the utilities of the\\n``sklearn.feature_extraction.text`` as demonstrated in the following\\nexample that extract `TF-IDF`_ vectors of unigram tokens\\nfrom a subset of 20news::\\n\\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\\n  >>> categories = [\\'alt.atheism\\', \\'talk.religion.misc\\',\\n  ...               \\'comp.graphics\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       categories=categories)\\n  >>> vectorizer = TfidfVectorizer()\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> vectors.shape\\n  (2034, 34118)\\n\\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\\ncomponents by sample in a more than 30000-dimensional space\\n(less than .5% non-zero features)::\\n\\n  >>> vectors.nnz / float(vectors.shape[0])\\n  159.01327...\\n\\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \\nreturns ready-to-use token counts features instead of file names.\\n\\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\\n\\n\\nFiltering text for more realistic training\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIt is easy for a classifier to overfit on particular things that appear in the\\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\\nhigh F-scores, but their results would not generalize to other documents that\\naren\\'t from this window of time.\\n\\nFor example, let\\'s look at the results of a multinomial Naive Bayes classifier,\\nwhich is fast to train and achieves a decent F-score::\\n\\n  >>> from sklearn.naive_bayes import MultinomialNB\\n  >>> from sklearn import metrics\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.88213...\\n\\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\\nthe training and test data, instead of segmenting by time, and in that case\\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\\nyet of what\\'s going on inside this classifier?)\\n\\nLet\\'s take a look at what the most informative features are:\\n\\n  >>> import numpy as np\\n  >>> def show_top10(classifier, vectorizer, categories):\\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\\n  ...     for i, category in enumerate(categories):\\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\\n  ...\\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\\n  alt.atheism: edu it and in you that is of to the\\n  comp.graphics: edu in graphics it is for and of to the\\n  sci.space: edu it that is in and space to of the\\n  talk.religion.misc: not it you in is that and to of the\\n\\n\\nYou can now see many things that these features have overfit to:\\n\\n- Almost every group is distinguished by whether headers such as\\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\\n- Another significant feature involves whether the sender is affiliated with\\n  a university, as indicated either by their headers or their signature.\\n- The word \"article\" is a significant feature, based on how often people quote\\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\\n  wrote:\"\\n- Other features match the names and e-mail addresses of particular people who\\n  were posting at the time.\\n\\nWith such an abundance of clues that distinguish newsgroups, the classifiers\\nbarely have to identify topics from text at all, and they all perform at the\\nsame high level.\\n\\nFor this reason, the functions that load 20 Newsgroups data provide a\\nparameter called **remove**, telling it what kinds of information to strip out\\nof each file. **remove** should be a tuple containing any subset of\\n``(\\'headers\\', \\'footers\\', \\'quotes\\')``, telling it to remove headers, signature\\nblocks, and quotation blocks respectively.\\n\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(pred, newsgroups_test.target, average=\\'macro\\')\\n  0.77310...\\n\\nThis classifier lost over a lot of its F-score, just because we removed\\nmetadata that has little to do with topic classification.\\nIt loses even more if we also strip this metadata from the training data:\\n\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                       categories=categories)\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.76995...\\n\\nSome other classifiers cope better with this harder version of the task. Try\\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\\nthe ``--filter`` option to compare the results.\\n\\n.. topic:: Recommendation\\n\\n  When evaluating text classifiers on the 20 Newsgroups data, you\\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\\n  setting ``remove=(\\'headers\\', \\'footers\\', \\'quotes\\')``. The F-score will be\\n  lower because it is more realistic.\\n\\n.. topic:: Examples\\n\\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\\n\\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n'}\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n20\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)\n",
    "print(len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7 4 4 ... 3 1 8]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target)\n",
    "print(list(set(dataset.target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".. _20newsgroups_dataset:\n\nThe 20 newsgroups text dataset\n------------------------------\n\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\n20 topics split in two subsets: one for training (or development)\nand the other one for testing (or for performance evaluation). The split\nbetween the train and test set is based upon a messages posted before\nand after a specific date.\n\nThis module contains two loaders. The first one,\n:func:`sklearn.datasets.fetch_20newsgroups`,\nreturns a list of the raw texts that can be fed to text feature\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\nwith custom parameters so as to extract feature vectors.\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\nreturns ready-to-use features, i.e., it is not necessary to use a feature\nextractor.\n\n**Data Set Characteristics:**\n\n    =================   ==========\n    Classes                     20\n    Samples total            18846\n    Dimensionality               1\n    Features                  text\n    =================   ==========\n\nUsage\n~~~~~\n\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\nfetching / caching functions that downloads the data archive from\nthe original `20 newsgroups website`_, extracts the archive contents\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\n:func:`sklearn.datasets.load_files` on either the training or\ntesting set folder, or both of them::\n\n  >>> from sklearn.datasets import fetch_20newsgroups\n  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n  >>> from pprint import pprint\n  >>> pprint(list(newsgroups_train.target_names))\n  ['alt.atheism',\n   'comp.graphics',\n   'comp.os.ms-windows.misc',\n   'comp.sys.ibm.pc.hardware',\n   'comp.sys.mac.hardware',\n   'comp.windows.x',\n   'misc.forsale',\n   'rec.autos',\n   'rec.motorcycles',\n   'rec.sport.baseball',\n   'rec.sport.hockey',\n   'sci.crypt',\n   'sci.electronics',\n   'sci.med',\n   'sci.space',\n   'soc.religion.christian',\n   'talk.politics.guns',\n   'talk.politics.mideast',\n   'talk.politics.misc',\n   'talk.religion.misc']\n\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\nattribute is the integer index of the category::\n\n  >>> newsgroups_train.filenames.shape\n  (11314,)\n  >>> newsgroups_train.target.shape\n  (11314,)\n  >>> newsgroups_train.target[:10]\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n\nIt is possible to load only a sub-selection of the categories by passing the\nlist of the categories to load to the\n:func:`sklearn.datasets.fetch_20newsgroups` function::\n\n  >>> cats = ['alt.atheism', 'sci.space']\n  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n  >>> list(newsgroups_train.target_names)\n  ['alt.atheism', 'sci.space']\n  >>> newsgroups_train.filenames.shape\n  (1073,)\n  >>> newsgroups_train.target.shape\n  (1073,)\n  >>> newsgroups_train.target[:10]\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n\nConverting text to vectors\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn order to feed predictive or clustering models with the text data,\none first need to turn the text into vectors of numerical values suitable\nfor statistical analysis. This can be achieved with the utilities of the\n``sklearn.feature_extraction.text`` as demonstrated in the following\nexample that extract `TF-IDF`_ vectors of unigram tokens\nfrom a subset of 20news::\n\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n  >>> categories = ['alt.atheism', 'talk.religion.misc',\n  ...               'comp.graphics', 'sci.space']\n  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n  ...                                       categories=categories)\n  >>> vectorizer = TfidfVectorizer()\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n  >>> vectors.shape\n  (2034, 34118)\n\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\ncomponents by sample in a more than 30000-dimensional space\n(less than .5% non-zero features)::\n\n  >>> vectors.nnz / float(vectors.shape[0])\n  159.01327...\n\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \nreturns ready-to-use token counts features instead of file names.\n\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n\n\nFiltering text for more realistic training\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is easy for a classifier to overfit on particular things that appear in the\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\nhigh F-scores, but their results would not generalize to other documents that\naren't from this window of time.\n\nFor example, let's look at the results of a multinomial Naive Bayes classifier,\nwhich is fast to train and achieves a decent F-score::\n\n  >>> from sklearn.naive_bayes import MultinomialNB\n  >>> from sklearn import metrics\n  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n  ...                                      categories=categories)\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n  >>> clf = MultinomialNB(alpha=.01)\n  >>> clf.fit(vectors, newsgroups_train.target)\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n  >>> pred = clf.predict(vectors_test)\n  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n  0.88213...\n\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\nthe training and test data, instead of segmenting by time, and in that case\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\nyet of what's going on inside this classifier?)\n\nLet's take a look at what the most informative features are:\n\n  >>> import numpy as np\n  >>> def show_top10(classifier, vectorizer, categories):\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n  ...     for i, category in enumerate(categories):\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n  ...\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n  alt.atheism: edu it and in you that is of to the\n  comp.graphics: edu in graphics it is for and of to the\n  sci.space: edu it that is in and space to of the\n  talk.religion.misc: not it you in is that and to of the\n\n\nYou can now see many things that these features have overfit to:\n\n- Almost every group is distinguished by whether headers such as\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n- Another significant feature involves whether the sender is affiliated with\n  a university, as indicated either by their headers or their signature.\n- The word \"article\" is a significant feature, based on how often people quote\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n  wrote:\"\n- Other features match the names and e-mail addresses of particular people who\n  were posting at the time.\n\nWith such an abundance of clues that distinguish newsgroups, the classifiers\nbarely have to identify topics from text at all, and they all perform at the\nsame high level.\n\nFor this reason, the functions that load 20 Newsgroups data provide a\nparameter called **remove**, telling it what kinds of information to strip out\nof each file. **remove** should be a tuple containing any subset of\n``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\nblocks, and quotation blocks respectively.\n\n  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n  ...                                      remove=('headers', 'footers', 'quotes'),\n  ...                                      categories=categories)\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n  >>> pred = clf.predict(vectors_test)\n  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n  0.77310...\n\nThis classifier lost over a lot of its F-score, just because we removed\nmetadata that has little to do with topic classification.\nIt loses even more if we also strip this metadata from the training data:\n\n  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n  ...                                       remove=('headers', 'footers', 'quotes'),\n  ...                                       categories=categories)\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n  >>> clf = MultinomialNB(alpha=.01)\n  >>> clf.fit(vectors, newsgroups_train.target)\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n  >>> pred = clf.predict(vectors_test)\n  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n  0.76995...\n\nSome other classifiers cope better with this harder version of the task. Try\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\nthe ``--filter`` option to compare the results.\n\n.. topic:: Recommendation\n\n  When evaluating text classifiers on the 20 Newsgroups data, you\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n  lower because it is more realistic.\n\n.. topic:: Examples\n\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n\n"
     ]
    }
   ],
   "source": [
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\nSubject: SI Clock Poll - Final Call\nSummary: Final call for SI clock reports\nKeywords: SI,acceleration,clock,upgrade\nArticle-I.D.: shelley.1qvfo9INNc3s\nOrganization: University of Washington\nLines: 11\nNNTP-Posting-Host: carson.u.washington.edu\n\nA fair number of brave souls who upgraded their SI clock oscillator have\nshared their experiences for this poll. Please send a brief message detailing\nyour experiences with the procedure. Top speed attained, CPU rated speed,\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\nfunctionality with 800 and 1.4 m floppies are especially requested.\n\nI will be summarizing in the next two days, so please add to the network\nknowledge base if you have done the clock upgrade and haven't answered this\npoll. Thanks.\n\nGuy Kuo <guykuo@u.washington.edu>\n\n<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data[1])\n",
    "print(type(dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A fair number of brave souls who upgraded their SI clock oscillator have\nshared their experiences for this poll. Please send a brief message detailing\nyour experiences with the procedure. Top speed attained, CPU rated speed,\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\nfunctionality with 800 and 1.4 m floppies are especially requested.\n\nI will be summarizing in the next two days, so please add to the network\nknowledge base if you have done the clock upgrade and haven't answered this\npoll. Thanks.\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "print(dataset.data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text\n",
       "0      I was wondering if anyone out there could enli...\n",
       "1      A fair number of brave souls who upgraded thei...\n",
       "2      well folks, my mac plus finally gave up the gh...\n",
       "3      \\nDo you have Weitek's address/phone number?  ...\n",
       "4      From article <C5owCB.n3p@world.std.com>, by to...\n",
       "...                                                  ...\n",
       "11309  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...\n",
       "11310  I have a (very old) Mac 512k and a Mac Plus, b...\n",
       "11311  I just installed a DX2-66 CPU in a clone mothe...\n",
       "11312  \\nWouldn't this require a hyper-sphere.  In 3-...\n",
       "11313  Stolen from Pasadena between 4:30 and 6:30 pm ...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I was wondering if anyone out there could enli...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A fair number of brave souls who upgraded thei...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>well folks, my mac plus finally gave up the gh...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>I have a (very old) Mac 512k and a Mac Plus, b...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>Stolen from Pasadena between 4:30 and 6:30 pm ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "text_df = pd.DataFrame(dataset.data, columns=['text'], index=None)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text\n",
       "0      i was wondering if anyone out there could enli...\n",
       "1      a fair number of brave souls who upgraded thei...\n",
       "2      well folks, my mac plus finally gave up the gh...\n",
       "3      \\ndo you have weitek's address/phone number?  ...\n",
       "4      from article <c5owcb.n3p@world.std.com>, by to...\n",
       "...                                                  ...\n",
       "11309  dn> from: nyeda@cnsvax.uwec.edu (david nye)\\nd...\n",
       "11310  i have a (very old) mac 512k and a mac plus, b...\n",
       "11311  i just installed a dx2-66 cpu in a clone mothe...\n",
       "11312  \\nwouldn't this require a hyper-sphere.  in 3-...\n",
       "11313  stolen from pasadena between 4:30 and 6:30 pm ...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i was wondering if anyone out there could enli...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a fair number of brave souls who upgraded thei...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>well folks, my mac plus finally gave up the gh...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\ndo you have weitek's address/phone number?  ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>from article &lt;c5owcb.n3p@world.std.com&gt;, by to...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>dn&gt; from: nyeda@cnsvax.uwec.edu (david nye)\\nd...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>i have a (very old) mac 512k and a mac plus, b...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>i just installed a dx2-66 cpu in a clone mothe...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>\\nwouldn't this require a hyper-sphere.  in 3-...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>stolen from pasadena between 4:30 and 6:30 pm ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "text_df.text = text_df.apply(lambda row: row['text'].lower(), axis = 1)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text\n",
       "0      [i, was, wondering, if, anyone, out, there, co...\n",
       "1      [a, fair, number, of, brave, souls, who, upgra...\n",
       "2      [well, folks, ,, my, mac, plus, finally, gave,...\n",
       "3      [do, you, have, weitek, 's, address/phone, num...\n",
       "4      [from, article, <, c5owcb.n3p, @, world.std.co...\n",
       "...                                                  ...\n",
       "11309  [dn, >, from, :, nyeda, @, cnsvax.uwec.edu, (,...\n",
       "11310  [i, have, a, (, very, old, ), mac, 512k, and, ...\n",
       "11311  [i, just, installed, a, dx2-66, cpu, in, a, cl...\n",
       "11312  [would, n't, this, require, a, hyper-sphere, ....\n",
       "11313  [stolen, from, pasadena, between, 4:30, and, 6...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[i, was, wondering, if, anyone, out, there, co...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[a, fair, number, of, brave, souls, who, upgra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[well, folks, ,, my, mac, plus, finally, gave,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[do, you, have, weitek, 's, address/phone, num...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[from, article, &lt;, c5owcb.n3p, @, world.std.co...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>[dn, &gt;, from, :, nyeda, @, cnsvax.uwec.edu, (,...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>[i, have, a, (, very, old, ), mac, 512k, and, ...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>[i, just, installed, a, dx2-66, cpu, in, a, cl...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>[would, n't, this, require, a, hyper-sphere, ....</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>[stolen, from, pasadena, between, 4:30, and, 6...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_df.text = text_df.apply(lambda row: word_tokenize(row['text']), axis = 1)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                    text\n0      [wondering, anyone, could, enlighten, car, saw...\n1      [fair, number, brave, souls, upgraded, si, clo...\n2      [well, folks, ,, mac, plus, finally, gave, gho...\n3      [weitek, 's, address/phone, number, ?, 'd, lik...\n4      [article, <, c5owcb.n3p, @, world.std.com, >, ...\n...                                                  ...\n11309  [dn, >, :, nyeda, @, cnsvax.uwec.edu, (, david...\n11310  [(, old, ), mac, 512k, mac, plus, ,, problem, ...\n11311  [installed, dx2-66, cpu, clone, motherboard, ,...\n11312  [would, n't, require, hyper-sphere, ., 3-space...\n11313  [stolen, pasadena, 4:30, 6:30, pm, 4/15, ., bl...\n\n[11314 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "text_df.text = text_df.text.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text\n",
       "0      [wondering, anyone, could, enlighten, car, saw...\n",
       "1      [fair, number, brave, souls, upgraded, si, clo...\n",
       "2      [well, folks, mac, plus, finally, gave, ghost,...\n",
       "3         [weitek, number, like, get, information, chip]\n",
       "4      [article, tombaker, tom, baker, understanding,...\n",
       "...                                                  ...\n",
       "11309  [dn, nyeda, david, nye, dn, neurology, dn, con...\n",
       "11310  [old, mac, mac, plus, problem, screens, blank,...\n",
       "11311  [installed, cpu, clone, motherboard, tried, mo...\n",
       "11312  [would, require, points, specifies, sphere, fa...\n",
       "11313  [stolen, pasadena, pm, blue, white, honda, cal...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[wondering, anyone, could, enlighten, car, saw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[fair, number, brave, souls, upgraded, si, clo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[well, folks, mac, plus, finally, gave, ghost,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[weitek, number, like, get, information, chip]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[article, tombaker, tom, baker, understanding,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>[dn, nyeda, david, nye, dn, neurology, dn, con...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>[old, mac, mac, plus, problem, screens, blank,...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>[installed, cpu, clone, motherboard, tried, mo...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>[would, require, points, specifies, sphere, fa...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>[stolen, pasadena, pm, blue, white, honda, cal...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "text_df.text = text_df.text.apply(lambda row: [word for word in row if word.isalpha()])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text\n",
       "0      [wondering, anyone, could, enlighten, car, saw...\n",
       "1      [fair, number, brave, soul, upgraded, si, cloc...\n",
       "2      [well, folk, mac, plus, finally, gave, ghost, ...\n",
       "3         [weitek, number, like, get, information, chip]\n",
       "4      [article, tombaker, tom, baker, understanding,...\n",
       "...                                                  ...\n",
       "11309  [dn, nyeda, david, nye, dn, neurology, dn, con...\n",
       "11310  [old, mac, mac, plus, problem, screen, blank, ...\n",
       "11311  [installed, cpu, clone, motherboard, tried, mo...\n",
       "11312  [would, require, point, specifies, sphere, far...\n",
       "11313  [stolen, pasadena, pm, blue, white, honda, cal...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[wondering, anyone, could, enlighten, car, saw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[fair, number, brave, soul, upgraded, si, cloc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[well, folk, mac, plus, finally, gave, ghost, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[weitek, number, like, get, information, chip]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[article, tombaker, tom, baker, understanding,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>[dn, nyeda, david, nye, dn, neurology, dn, con...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>[old, mac, mac, plus, problem, screen, blank, ...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>[installed, cpu, clone, motherboard, tried, mo...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>[would, require, point, specifies, sphere, far...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>[stolen, pasadena, pm, blue, white, honda, cal...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text_df.text = text_df.text.apply(lambda row: [lemmatizer.lemmatize(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text\n",
       "0      wondering anyone could enlighten car saw day s...\n",
       "1      fair number brave soul upgraded si clock oscil...\n",
       "2      well folk mac plus finally gave ghost weekend ...\n",
       "3                weitek number like get information chip\n",
       "4      article tombaker tom baker understanding error...\n",
       "...                                                  ...\n",
       "11309  dn nyeda david nye dn neurology dn consultatio...\n",
       "11310  old mac mac plus problem screen blank sometime...\n",
       "11311  installed cpu clone motherboard tried mounting...\n",
       "11312  would require point specifies sphere far see u...\n",
       "11313  stolen pasadena pm blue white honda california...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wondering anyone could enlighten car saw day s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fair number brave soul upgraded si clock oscil...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>well folk mac plus finally gave ghost weekend ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>weitek number like get information chip</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>article tombaker tom baker understanding error...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>dn nyeda david nye dn neurology dn consultatio...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>old mac mac plus problem screen blank sometime...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>installed cpu clone motherboard tried mounting...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>would require point specifies sphere far see u...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>stolen pasadena pm blue white honda california...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "text_df['text'] = text_df.apply(lambda row: ' '.join(row['text']), axis = 1)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8485, 1)\n(2829, 1)\n(8485,)\n(2829,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_df, dataset.target, test_size = 0.25, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8485, 48239)\n(2829, 48239)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['text'], dataset.target)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6868151290208554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "print(model.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "source": [
    "I should not add data to a dataframe and do it without it perhaps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}